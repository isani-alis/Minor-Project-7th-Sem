{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd23e8a5-efd9-4e2b-9f04-6473beb19688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d91194f-c74d-4961-a9b6-a267deaa2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flow_stack(u_dir, v_dir, idx, stack_len=5, name_pattern=\"frame_{:05d}.jpg\"):\n",
    "    \"\"\"\n",
    "    Return numpy array of shape (10, H, W) -> 5 u followed by 5 v (grayscale)\n",
    "    idx is the *frame index* (zero-based) corresponding to the last frame in the stack.\n",
    "    \"\"\"\n",
    "    u_imgs = []\n",
    "    v_imgs = []\n",
    "    for i in range(idx - stack_len + 1, idx + 1):\n",
    "        fname = name_pattern.format(i)\n",
    "        u_path = os.path.join(u_dir, fname)\n",
    "        v_path = os.path.join(v_dir, fname)\n",
    "        if not os.path.exists(u_path) or not os.path.exists(v_path):\n",
    "            return None\n",
    "        u_img = cv2.imread(u_path, cv2.IMREAD_GRAYSCALE)\n",
    "        v_img = cv2.imread(v_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if u_img is None or v_img is None:\n",
    "            return None\n",
    "        # ensure (H,W)\n",
    "        u_imgs.append(u_img[np.newaxis, ...])  # (1,H,W)\n",
    "        v_imgs.append(v_img[np.newaxis, ...])\n",
    "    stack = np.concatenate(u_imgs + v_imgs, axis=0).astype(np.float32)  # (10,H,W)\n",
    "    return stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d60823-5c6b-49c3-9a8f-fe1524234f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=10, out_dim=1024, backbone=\"resnet50\"):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_dim = out_dim\n",
    "        self.backbone_name = backbone\n",
    "\n",
    "        if backbone == \"resnet50\":\n",
    "            # robust fallback: ResNet50 + project\n",
    "            net = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "            # adapt first conv to in_channels\n",
    "            old_conv = net.conv1\n",
    "            new_conv = nn.Conv2d(in_channels, old_conv.out_channels,\n",
    "                                 kernel_size=old_conv.kernel_size,\n",
    "                                 stride=old_conv.stride,\n",
    "                                 padding=old_conv.padding,\n",
    "                                 bias=(old_conv.bias is not None))\n",
    "            # init new_conv weights by copying RGB weights and repeating/averaging\n",
    "            with torch.no_grad():\n",
    "                if in_channels >= 3:\n",
    "                    new_conv.weight[:, :3, :, :] = old_conv.weight\n",
    "                    # extra channels: copy average of RGB\n",
    "                    if in_channels > 3:\n",
    "                        mean_rgb = old_conv.weight.mean(dim=1, keepdim=True)  # (out,1,k,k)\n",
    "                        for c in range(3, in_channels):\n",
    "                            new_conv.weight[:, c:c+1, :, :] = mean_rgb\n",
    "                else:\n",
    "                    # unlikely: in_channels < 3\n",
    "                    new_conv.weight[:, :in_channels, :, :] = old_conv.weight[:, :in_channels, :, :]\n",
    "            net.conv1 = new_conv\n",
    "\n",
    "            # remove classification head: use global avgpool output (2048)\n",
    "            net.fc = nn.Identity()  # so forward returns (B,2048) after avgpool\n",
    "            feat_dim = 2048\n",
    "            self.backbone = net\n",
    "            self.proj = nn.Linear(feat_dim, out_dim)\n",
    "        else:\n",
    "            raise ValueError(\"Backbone '%s' not implemented in this script\" % backbone)\n",
    "\n",
    "        # freeze backbone (paper: features fixed)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "        # allow proj to be trainable? Paper keeps extractor fixed; projection can be part of feature extraction\n",
    "        for p in self.proj.parameters():\n",
    "            p.requires_grad = False  # keep projection fixed too for strict adherence\n",
    "            # if you want to fine-tune proj set True\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, in_channels, H, W) float, normalized already\n",
    "        b = x.size(0)\n",
    "        feat = self.backbone(x)          # (B, feat_dim)\n",
    "        out = self.proj(feat)            # (B, out_dim)\n",
    "        return out\n",
    "\n",
    "# -----------------------------------\n",
    "# Extract flow features for one video (save .npy)\n",
    "# -----------------------------------\n",
    "def extract_flow_features_for_video(u_dir, v_dir, out_path,\n",
    "                                    extractor: FlowFeatureExtractor,\n",
    "                                    stack_len=5, stride=5,\n",
    "                                    resize=(256,456), device=\"cpu\",\n",
    "                                    name_pattern=\"frame_{:05d}.jpg\",\n",
    "                                    verbose=True):\n",
    "    \"\"\"\n",
    "    Walk frames in u_dir/v_dir and extract features. Save to out_path (.npy)\n",
    "    - resize is (H, W) = (256,456) as per paper (H=256, W=456).\n",
    "    - stride is sampling of last stack frame index (if sample_rate=5 the npy index ~ frame//5).\n",
    "    \"\"\"\n",
    "    files = sorted(os.listdir(u_dir))\n",
    "    n_frames = len(files)\n",
    "    feats = []\n",
    "    extractor = extractor.to(device).eval()\n",
    "\n",
    "    # precompute mean/std as tensors\n",
    "    mean = torch.tensor([0.485] * extractor.in_channels, device=device).view(1, extractor.in_channels, 1, 1)\n",
    "    std  = torch.tensor([0.229] * extractor.in_channels, device=device).view(1, extractor.in_channels, 1, 1)\n",
    "\n",
    "    for idx in range(stack_len - 1, n_frames, stride):\n",
    "        stack = load_flow_stack(u_dir, v_dir, idx, stack_len=stack_len, name_pattern=name_pattern)\n",
    "        if stack is None:\n",
    "            if verbose:\n",
    "                # missing frames at edges\n",
    "                pass\n",
    "            continue\n",
    "        # stack: (10, H_orig, W_orig)\n",
    "        t = torch.from_numpy(stack).unsqueeze(0).to(device)  # (1,10,H,W)\n",
    "        t = t.float() / 255.0\n",
    "        # resize to desired (H,W)\n",
    "        t = F.interpolate(t, size=resize, mode=\"bilinear\", align_corners=False)\n",
    "        # normalize\n",
    "        t = (t - mean) / std\n",
    "\n",
    "        with torch.no_grad():\n",
    "            f = extractor(t)  # (1, out_dim)\n",
    "        feats.append(f.cpu().numpy().reshape(-1))\n",
    "\n",
    "    feats = np.stack(feats, axis=0) if len(feats) > 0 else np.zeros((0, extractor.out_dim), dtype=np.float32)\n",
    "    np.save(out_path, feats)\n",
    "    if verbose:\n",
    "        print(f\"Saved features {feats.shape} -> {out_path}\")\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4f9165-4f9b-450b-9dbb-8b4ccf882a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpicFlowFeatureDataset(Dataset):\n",
    "    def __init__(self, annots, feature_dir, context=21, stride_on_features=1, label_key=\"action_class\"):\n",
    "        \"\"\"\n",
    "        annots: list of dicts with keys 'video_id', 'start_frame', ... (start_frame in raw video frame units)\n",
    "        feature_dir: folder containing per-video files like P01_01_flow.npy\n",
    "        context: number of snippets (features) to use (e.g. 21)\n",
    "        stride_on_features: if features were sampled at sample_rate s, then feature_index = frame // s.\n",
    "                            If your .npy already indexes sampled frames, set stride_on_features=1.\n",
    "        label_key: which annotation field to use (\"action_class\"/\"verb_class\"/\"noun_class\")\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.feature_dir = feature_dir\n",
    "        self.context = context\n",
    "        self.stride_on_features = stride_on_features\n",
    "        self.label_key = label_key\n",
    "\n",
    "        for a in annots:\n",
    "            vid = a[\"video_id\"]\n",
    "            start = int(a[\"start_frame\"])\n",
    "            label = int(a[self.label_key])\n",
    "            feat_path = os.path.join(feature_dir, f\"{vid}_flow.npy\")\n",
    "            if not os.path.exists(feat_path):\n",
    "                continue\n",
    "            self.samples.append({\n",
    "                \"vid\": vid, \"start_frame\": start, \"feat_path\": feat_path, \"label\": label\n",
    "            })\n",
    "\n",
    "        # cache loaded feature arrays\n",
    "        self._cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        vid = s[\"vid\"]\n",
    "        if vid not in self._cache:\n",
    "            self._cache[vid] = np.load(s[\"feat_path\"])   # (T_feats, D)\n",
    "        feats = self._cache[vid]\n",
    "        feat_index = s[\"start_frame\"] // self.stride_on_features\n",
    "        feat_index = min(feat_index, len(feats))  # clamp\n",
    "        start_idx = max(0, feat_index - self.context)\n",
    "        seq = feats[start_idx:feat_index]   # (k, D)\n",
    "        if seq.shape[0] < self.context:\n",
    "            pad = np.zeros((self.context - seq.shape[0], feats.shape[1]), dtype=np.float32)\n",
    "            seq = np.vstack([pad, seq])\n",
    "        return torch.from_numpy(seq).float(), torch.tensor(s[\"label\"]).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a78ab2-f355-4f79-8c1b-7f75cecb9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualTemporalBlock(nn.Module):\n",
    "    def __init__(self, C, kernel=3, dilation=1, p_drop=0.3):\n",
    "        super().__init__()\n",
    "        pad = (kernel - 1) * dilation\n",
    "        self.conv = nn.Conv1d(C, C, kernel, padding=pad, dilation=dilation)\n",
    "        self.bn = nn.BatchNorm1d(C)\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.drop(out)\n",
    "        # trim right padding to keep length: conv with padding=(k-1)*d returns len + pad, so slice last T\n",
    "        out = out[..., :x.size(2)]\n",
    "        return self.relu(out + x)\n",
    "\n",
    "class UniFlowTCN(nn.Module):\n",
    "    def __init__(self, in_dim=1024, hidden=1024, n_layers=4, kernel=3,\n",
    "                 n_action=200, n_verb=None, n_noun=None, p_drop=0.3):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Conv1d(in_dim, hidden, kernel_size=1)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualTemporalBlock(hidden, kernel=kernel, dilation=(i+1), p_drop=p_drop)\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "        # heads (global pooling over time)\n",
    "        self.head_action = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Dropout(0.7), nn.Linear(hidden, n_action))\n",
    "        # optional verb/noun\n",
    "        self.head_verb = None\n",
    "        self.head_noun = None\n",
    "        if n_verb:\n",
    "            self.head_verb = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Dropout(0.7), nn.Linear(hidden, n_verb))\n",
    "        if n_noun:\n",
    "            self.head_noun = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Dropout(0.7), nn.Linear(hidden, n_noun))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        x = x.permute(0, 2, 1)  # (B, D, T)\n",
    "        x = self.input_proj(x)  # (B, hidden, T)\n",
    "        for b in self.blocks:\n",
    "            x = b(x)\n",
    "        outA = self.head_action(x)\n",
    "        outV, outN = None, None\n",
    "        if self.head_verb:\n",
    "            outV = self.head_verb(x)\n",
    "        if self.head_noun:\n",
    "            outN = self.head_noun(x)\n",
    "        return outA, outV, outN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e655512f-5414-4441-b188-0659e69c3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unimodal_flow(annots, feature_root, num_actions,\n",
    "                        context=21, batch_size=16, epochs=30,\n",
    "                        lr=5e-4, weight_decay=5e-4, stride_on_features=1,\n",
    "                        device=None):\n",
    "    device = torch.device(device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    ds = EpicFlowFeatureDataset(annots, feature_root, context=context, stride_on_features=stride_on_features, label_key=\"action_class\")\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    model = UniFlowTCN(in_dim=1024, hidden=1024, n_layers=4, n_action=num_actions).to(device)\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total, correct, loss_sum = 0.0, 0, 0.0\n",
    "        for X, y in loader:\n",
    "            X = X.to(device)   # (B,T,1024)\n",
    "            y = y.to(device)\n",
    "            outA,_,_ = model(X)\n",
    "            loss = ce(outA, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            preds = outA.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        print(f\"Epoch {ep}/{epochs} loss={loss_sum/len(loader):.4f} acc={correct/total:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe2f933-5b71-4d1a-97cc-fa966b06c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features (9913, 1024) -> P01_01_flow.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.36699796, -0.09396664,  0.04978663, ..., -0.01747663,\n",
       "         0.22542918,  0.10597148],\n",
       "       [ 0.39778498, -0.13228619,  0.09689356, ..., -0.04566773,\n",
       "         0.23897809,  0.05014143],\n",
       "       [ 0.39031318, -0.14413048,  0.01582648, ..., -0.01113382,\n",
       "         0.29390094,  0.02070261],\n",
       "       ...,\n",
       "       [ 0.39688268,  0.046829  ,  0.06360796, ..., -0.4679838 ,\n",
       "         0.02355651, -0.36303392],\n",
       "       [ 0.5092193 , -0.05146424,  0.13493755, ..., -0.3841746 ,\n",
       "         0.09903391, -0.4390804 ],\n",
       "       [ 0.1496426 ,  0.09781548,  0.19651009, ..., -0.14331797,\n",
       "         0.16697614, -0.20242082]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_dir=r\"C:\\Users\\Alis\\Desktop\\Minor Project\\OpticalFlow\\P01_01\\u\"\n",
    "v_dir=r\"C:\\Users\\Alis\\Desktop\\Minor Project\\OpticalFlow\\P01_01\\v\"\n",
    "extractor = FlowFeatureExtractor(in_channels=10, out_dim=1024, backbone=\"resnet50\")\n",
    "extract_flow_features_for_video(u_dir, v_dir, out_path=\"P01_01_flow.npy\", extractor=extractor,\n",
    "                                stack_len=5, stride=5, resize=(256,456), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b94de9c4-8423-450d-b212-303bec3f448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features (3013, 1024) -> P01_02_flow.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.15611236, -0.10960457, -0.0683035 , ...,  0.22482777,\n",
       "        -0.3398625 ,  0.14191732],\n",
       "       [ 0.2840967 , -0.05636885, -0.19736274, ...,  0.26885965,\n",
       "        -0.37536326,  0.30609125],\n",
       "       [ 0.05536053, -0.05508311, -0.07395775, ...,  0.24293973,\n",
       "        -0.25257096,  0.12840156],\n",
       "       ...,\n",
       "       [ 0.08066492, -0.20348191, -0.16416739, ...,  0.29985347,\n",
       "        -0.19633481,  0.04107007],\n",
       "       [ 0.03719174, -0.10905169,  0.00930319, ...,  0.12232547,\n",
       "        -0.01758666,  0.05853077],\n",
       "       [ 0.12534976, -0.18267441, -0.14506426, ...,  0.07100389,\n",
       "        -0.20469187,  0.18305536]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_dir=r\"C:\\Users\\Alis\\Desktop\\Minor Project\\OpticalFlow\\P01_02\\u\"\n",
    "v_dir=r\"C:\\Users\\Alis\\Desktop\\Minor Project\\OpticalFlow\\P01_02\\v\"\n",
    "extractor = FlowFeatureExtractor(in_channels=10, out_dim=1024, backbone=\"resnet50\")\n",
    "extract_flow_features_for_video(u_dir, v_dir, out_path=\"P01_02_flow.npy\", extractor=extractor,\n",
    "                                stack_len=5, stride=5, resize=(256,456), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93a332ae-86a5-4961-8826-70a2fcd9f08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features (713, 1024) -> P01_03_flow.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.05461931,  0.14494097, -0.3226608 , ...,  0.09361984,\n",
       "        -0.10034194,  0.19786872],\n",
       "       [ 0.15957429,  0.21602915, -0.34196556, ...,  0.12058246,\n",
       "        -0.2081184 ,  0.14731336],\n",
       "       [ 0.12522033,  0.15050147, -0.27202994, ...,  0.16045268,\n",
       "        -0.16295923,  0.32625225],\n",
       "       ...,\n",
       "       [ 0.127799  , -0.13002308, -0.16407773, ...,  0.0600653 ,\n",
       "        -0.1542764 ,  0.2600136 ],\n",
       "       [ 0.14310129,  0.06494825,  0.0255594 , ..., -0.08432195,\n",
       "        -0.13418433,  0.16121985],\n",
       "       [ 0.07974368,  0.0647414 , -0.16003482, ..., -0.07082683,\n",
       "        -0.08923995,  0.32279852]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_dir=r\"C:\\Users\\Alis\\Desktop\\Minor Project\\OpticalFlow\\P01_03\\u\"\n",
    "v_dir=r\"C:\\Users\\Alis\\Desktop\\Minor Project\\OpticalFlow\\P01_03\\v\"\n",
    "extractor = FlowFeatureExtractor(in_channels=10, out_dim=1024, backbone=\"resnet50\")\n",
    "extract_flow_features_for_video(u_dir, v_dir, out_path=\"P01_03_flow.npy\", extractor=extractor,\n",
    "                                stack_len=5, stride=5, resize=(256,456), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d9d9f8b-d225-49f1-b087-01b399bc6987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features (631, 1024) -> P01_04_flow.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.19723895, -0.24409574,  0.01781794, ...,  0.21769232,\n",
       "         0.01159016,  0.20624839],\n",
       "       [ 0.15788816, -0.09464243,  0.04870874, ...,  0.26431003,\n",
       "        -0.07470681,  0.20641541],\n",
       "       [ 0.21049494, -0.14488025,  0.2837513 , ...,  0.23482522,\n",
       "        -0.16149665,  0.3392455 ],\n",
       "       ...,\n",
       "       [ 0.13354185, -0.23806041, -0.09902406, ...,  0.09224624,\n",
       "         0.0520287 ,  0.32973006],\n",
       "       [ 0.16996531, -0.31401446, -0.02806909, ...,  0.07524844,\n",
       "         0.08955118,  0.36377543],\n",
       "       [ 0.17643633, -0.23866682, -0.00228741, ...,  0.17315897,\n",
       "        -0.00053515,  0.15044698]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_dir=r\"C:\\Users\\Alis\\Desktop\\Minor Project\\OpticalFlow\\P01_04\\u\"\n",
    "v_dir=r\"C:\\Users\\Alis\\Desktop\\Minor Project\\OpticalFlow\\P01_04\\v\"\n",
    "extractor = FlowFeatureExtractor(in_channels=10, out_dim=1024, backbone=\"resnet50\")\n",
    "extract_flow_features_for_video(u_dir, v_dir, out_path=\"P01_04_flow.npy\", extractor=extractor,\n",
    "                                stack_len=5, stride=5, resize=(256,456), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15b2b4f0-6978-418c-8fe6-65d16b5fe2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features (7632, 1024) -> P01_05_flow.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.2694452 ,  0.3494546 , -0.00386948, ..., -0.1387239 ,\n",
       "        -0.39857176, -0.24953668],\n",
       "       [ 0.28278184,  0.09898116,  0.17627755, ..., -0.2428058 ,\n",
       "        -0.41330972, -0.1611986 ],\n",
       "       [ 0.2520699 ,  0.11901359,  0.06085365, ..., -0.21500331,\n",
       "        -0.45165324, -0.32585177],\n",
       "       ...,\n",
       "       [ 0.23219942,  0.12484025,  0.00754249, ..., -0.25584665,\n",
       "        -0.3017678 , -0.41445315],\n",
       "       [ 0.02810715,  0.03071283, -0.09522419, ..., -0.25188228,\n",
       "        -0.32147995, -0.2561596 ],\n",
       "       [ 0.01017908,  0.0276692 , -0.04355564, ..., -0.24570741,\n",
       "        -0.24765125, -0.2583301 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_dir=r\"C:\\Users\\Alis\\Desktop\\Minor Project\\OpticalFlow\\P01_05\\u\"\n",
    "v_dir=r\"C:\\Users\\Alis\\Desktop\\Minor Project\\OpticalFlow\\P01_05\\v\"\n",
    "extractor = FlowFeatureExtractor(in_channels=10, out_dim=1024, backbone=\"resnet50\")\n",
    "extract_flow_features_for_video(u_dir, v_dir, out_path=\"P01_05_flow.npy\", extractor=extractor,\n",
    "                                stack_len=5, stride=5, resize=(256,456), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b757d64-3fa1-4c51-8a74-a6c317a97a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0b54de3-271a-41f7-bb9d-56ea6622a70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   StartFrame  EndFrame     Verb  Verb_class    Noun  Noun_class  \\\n",
      "0           8       202     open           2    door           8   \n",
      "1         262       370  turn-on          12   light         113   \n",
      "2         418       569    close           3    door           8   \n",
      "3         766       839     open           2  fridge          10   \n",
      "4         915       983     take           0  celery         185   \n",
      "\n",
      "   Action_class     ActionName video_id  \n",
      "0             0      open door   P01_01  \n",
      "1             1  turn-on light   P01_01  \n",
      "2             2     close door   P01_01  \n",
      "3             3    open fridge   P01_01  \n",
      "4             4    take celery   P01_01  \n"
     ]
    }
   ],
   "source": [
    "label_dir = r\"C:\\Users\\Alis\\Desktop\\Minor Project\\Label\"  # adjust path if needed\n",
    "\n",
    "all_annots = []\n",
    "\n",
    "for fname in os.listdir(label_dir):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        video_id = fname.replace(\".csv\", \"\")  # e.g., P01_01\n",
    "        df = pd.read_csv(os.path.join(label_dir, fname))\n",
    "\n",
    "        # add video_id column\n",
    "        df[\"video_id\"] = video_id\n",
    "\n",
    "        all_annots.append(df)\n",
    "\n",
    "# merge into one dataframe\n",
    "df_all = pd.concat(all_annots, ignore_index=True)\n",
    "print(df_all.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08613da5-b0c9-4b00-9771-598ec0554c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotations: 545\n",
      "Number of unique actions: 114\n",
      "Example annotation: {'video_id': 'P01_01', 'start_frame': 8, 'action_class': 0, 'verb_class': 2, 'noun_class': 8}\n"
     ]
    }
   ],
   "source": [
    "annots = []\n",
    "\n",
    "for _, row in df_all.iterrows():\n",
    "    annots.append({\n",
    "        \"video_id\": row[\"video_id\"],\n",
    "        \"start_frame\": int(row[\"StartFrame\"]),\n",
    "        \"action_class\": int(row[\"Action_class\"]),\n",
    "        \"verb_class\": int(row[\"Verb_class\"]),\n",
    "        \"noun_class\": int(row[\"Noun_class\"]),\n",
    "    })\n",
    "\n",
    "num_actions = df_all[\"Action_class\"].nunique()\n",
    "\n",
    "print(\"Total annotations:\", len(annots))\n",
    "print(\"Number of unique actions:\", num_actions)\n",
    "print(\"Example annotation:\", annots[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f5b2e8e-a71a-4d34-b53e-576d60209ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 loss=5.3778 acc=0.0257\n",
      "Epoch 2/50 loss=5.0508 acc=0.0349\n",
      "Epoch 3/50 loss=4.8978 acc=0.0587\n",
      "Epoch 4/50 loss=4.6828 acc=0.0495\n",
      "Epoch 5/50 loss=4.6240 acc=0.0697\n",
      "Epoch 6/50 loss=4.4994 acc=0.0642\n",
      "Epoch 7/50 loss=4.4028 acc=0.0514\n",
      "Epoch 8/50 loss=4.4060 acc=0.0422\n",
      "Epoch 9/50 loss=4.3206 acc=0.0642\n",
      "Epoch 10/50 loss=4.2169 acc=0.0697\n",
      "Epoch 11/50 loss=4.1517 acc=0.0697\n",
      "Epoch 12/50 loss=4.1153 acc=0.0917\n",
      "Epoch 13/50 loss=3.9846 acc=0.0936\n",
      "Epoch 14/50 loss=4.1005 acc=0.0697\n",
      "Epoch 15/50 loss=4.0295 acc=0.0807\n",
      "Epoch 16/50 loss=3.9688 acc=0.0844\n",
      "Epoch 17/50 loss=3.9384 acc=0.0972\n",
      "Epoch 18/50 loss=3.8627 acc=0.1083\n",
      "Epoch 19/50 loss=3.8250 acc=0.0936\n",
      "Epoch 20/50 loss=3.8001 acc=0.0954\n",
      "Epoch 21/50 loss=3.8128 acc=0.0936\n",
      "Epoch 22/50 loss=3.7038 acc=0.1083\n",
      "Epoch 23/50 loss=3.7826 acc=0.1138\n",
      "Epoch 24/50 loss=3.6790 acc=0.1229\n",
      "Epoch 25/50 loss=3.5986 acc=0.1266\n",
      "Epoch 26/50 loss=3.5616 acc=0.1266\n",
      "Epoch 27/50 loss=3.4346 acc=0.1505\n",
      "Epoch 28/50 loss=3.5958 acc=0.1266\n",
      "Epoch 29/50 loss=3.5895 acc=0.1431\n",
      "Epoch 30/50 loss=3.4405 acc=0.1872\n",
      "Epoch 31/50 loss=3.4224 acc=0.1761\n",
      "Epoch 32/50 loss=3.4498 acc=0.1578\n",
      "Epoch 33/50 loss=3.2976 acc=0.1743\n",
      "Epoch 34/50 loss=3.3695 acc=0.1486\n",
      "Epoch 35/50 loss=3.3449 acc=0.1835\n",
      "Epoch 36/50 loss=3.2417 acc=0.2055\n",
      "Epoch 37/50 loss=3.2483 acc=0.1927\n",
      "Epoch 38/50 loss=3.1886 acc=0.2239\n",
      "Epoch 39/50 loss=3.2323 acc=0.2018\n",
      "Epoch 40/50 loss=3.1190 acc=0.2275\n",
      "Epoch 41/50 loss=3.1419 acc=0.1963\n",
      "Epoch 42/50 loss=3.0579 acc=0.2312\n",
      "Epoch 43/50 loss=3.0044 acc=0.2367\n",
      "Epoch 44/50 loss=3.0343 acc=0.2128\n",
      "Epoch 45/50 loss=3.0620 acc=0.2477\n",
      "Epoch 46/50 loss=2.9727 acc=0.2514\n",
      "Epoch 47/50 loss=2.9419 acc=0.2716\n",
      "Epoch 48/50 loss=2.8970 acc=0.2752\n",
      "Epoch 49/50 loss=2.8711 acc=0.2807\n",
      "Epoch 50/50 loss=2.9106 acc=0.2624\n"
     ]
    }
   ],
   "source": [
    "model = train_unimodal_flow(\n",
    "    annots,\n",
    "    feature_root=\"path_to_flow_npys\",  # update with actual path to your .npy flow features\n",
    "    num_actions=num_actions,\n",
    "    context=21,\n",
    "    batch_size=8,\n",
    "    epochs=50,\n",
    "    stride_on_features=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea4bdb01-d9af-41e2-9892-b63276a21c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 545\n",
      "One sample shape: torch.Size([21, 1024]) Label: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "dataset = EpicFlowFeatureDataset(\n",
    "    annots,\n",
    "    feature_dir=\"path_to_flow_npys\",  # update\n",
    "    context=21,\n",
    "    stride_on_features=5,\n",
    "    label_key=\"action_class\"\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(dataset))\n",
    "if len(dataset) > 0:\n",
    "    X, y = dataset[0]\n",
    "    print(\"One sample shape:\", X.shape, \"Label:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "224cb8a6-0ace-4aaa-8492-9383dba8b598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 259 test annotations\n",
      "Example: {'video_id': 'P01_05', 'start_frame': 248, 'end_frame': 355, 'verb': 'open', 'verb_class': 2, 'noun': 'fridge', 'noun_class': 10, 'action_name': 'open fridge', 'action_class': 0}\n"
     ]
    }
   ],
   "source": [
    "# path setup\n",
    "test_csv = r\"C:\\Users\\Alis\\Desktop\\Minor Project\\Label\\P01_05.csv\"\n",
    "test_feat = r\"C:\\Users\\Alis\\Desktop\\Minor Project\\path_to_flow_npys\"\n",
    "\n",
    "# load test annotations\n",
    "import pandas as pd\n",
    "\n",
    "test_annots = []\n",
    "video_id = \"P01_05\"\n",
    "df = pd.read_csv(test_csv)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    test_annots.append({\n",
    "        \"video_id\": video_id,\n",
    "        \"start_frame\": int(row[\"StartFrame\"]),\n",
    "        \"end_frame\": int(row[\"EndFrame\"]),\n",
    "        \"verb\": row[\"Verb\"],\n",
    "        \"verb_class\": int(row[\"Verb_class\"]),\n",
    "        \"noun\": row[\"Noun\"],\n",
    "        \"noun_class\": int(row[\"Noun_class\"]),\n",
    "        \"action_name\": row[\"ActionName\"],\n",
    "        \"action_class\": int(row[\"Action_class\"]),\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(test_annots)} test annotations\")\n",
    "print(\"Example:\", test_annots[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f1ab16b-e235-4af5-bc0a-51b37e1eb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "test_dataset = EpicFlowFeatureDataset(\n",
    "    test_annots,\n",
    "    feature_dir=test_feat,\n",
    "    context=21,\n",
    "    stride_on_features=5,\n",
    "    label_key=\"action_class\"\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12616800-d33a-437b-9759-67c9add4dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    preds_all, labels_all = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            out = model(X)\n",
    "\n",
    "            # handle models that return tuple (logits, extra)\n",
    "            if isinstance(out, tuple):\n",
    "                out = out[0]\n",
    "\n",
    "            _, preds = torch.max(out, 1)\n",
    "\n",
    "            total += y.size(0)\n",
    "            correct += (preds == y).sum().item()\n",
    "\n",
    "            preds_all.extend(preds.cpu().numpy())\n",
    "            labels_all.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    return preds_all, labels_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3904f4e1-2beb-4d55-996f-85ad15231f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0039\n"
     ]
    }
   ],
   "source": [
    "preds, labels = evaluate_model(model, test_loader, device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0f103e2-1fb3-457f-b088-a016cf15ef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# save only model weights (recommended)\n",
    "torch.save(model.state_dict(), \"tcn_flow_model.pth\")\n",
    "\n",
    "# OR save full model (not recommended if you plan to change class definition later)\n",
    "torch.save(model, \"tcn_flow_model_full.pth\")\n",
    "\n",
    "print(\"✅ Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72081ed6-6b71-42af-b325-dd35522eef54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
